{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a RAG to predit risk score of a transaction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiating model using groq inference server"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 2,

   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"Anish@hack1\"\n",
    "\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"deepseek-r1-distill-qwen-32b\", model_provider=\"groq\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple prompt with SystemMessage and HumanMessage"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<think>\\nAlright, so I need to figure out how to extract information from a transaction. Let me start by looking at the given example to understand what\\'s expected. The user provided a transaction with various fields like Transaction ID, Date, Payer, etc., and the assistant broke it down into categories: Transaction ID, Date, Payer, Payee, Amount, Payment Method, Purpose, Status, Location, and any additional notes.\\n\\nOkay, so if I were to approach this, I should first identify all the possible fields that a transaction might have. From the example, it looks like the main fields are ID, Date, Payer, Payee, Amount, Method, Purpose, and Status. But there might be more, like account numbers, banks involved, locations, etc.\\n\\nI should think about each part of the transaction. Let\\'s take the given example. The transaction ID is TXN20250322113045. That\\'s straightforward. The date is 2025-03-22T11:30:45Z, which includes the date and time in a specific format. Payer is Tesla, Inc., and their account number and bank. Similarly, the payee is Adani Green Energy Ltd., with their account details and bank.\\n\\nThe amount is 500,000,000 USD, so that\\'s the value. The payment method is Wire Transfer, which indicates how the transaction was made. The purpose is Investment in Renewable Energy Collaboration, which gives context about why the transaction happened. The status is Completed, showing the transaction\\'s current state.\\n\\nAdditionally, the assistant noted the locations of the banks involved, which adds geographical context. They also mentioned the transaction involves a cross-border wire transfer, which is an important detail.\\n\\nSo, when extracting information, I should consider:\\n\\n1. **Identifying the transaction ID**: This is usually a unique identifier for the transaction.\\n2. **Extracting the date and time**: This is crucial for tracking when the transaction occurred.\\n3. **Determining the payer and payee**: These are the entities involved in the transaction.\\n4. **Noting account numbers and banks**: This provides details about how the funds were transferred.\\n5. **Identifying the amount and currency**: This tells us the value of the transaction.\\n6. **Understanding the payment method**: This explains how the transaction was processed.\\n7. **Clarifying the purpose**: This gives context about why the transaction took place.\\n8. **Checking the status**: This indicates if the transaction was successful, pending, etc.\\n9. **Locating the geographical aspects**: This can involve the locations of the banks or entities.\\n10. **Highlighting additional notes**: Any extra information that adds context, like cross-border transfers.\\n\\nI should also consider edge cases. For instance, what if a transaction doesn\\'t have a specified purpose? Or if the payer and payee are in the same country? Maybe the payment method isn\\'t specified, or the status is pending. It\\'s important to handle all possible scenarios.\\n\\nAnother thought is about the structure of the data. In the example, the data was presented in a structured format with labels like \"Payer,\" \"Payee,\" etc. But in some cases, the data might be unstructured or presented differently. So, the extraction process might need to parse through the text to identify these fields.\\n\\nI should also think about validation. After extracting the data, it\\'s good practice to validate it. For example, checking if the transaction ID follows a certain format, ensuring that the date is in a correct format, and verifying that the account numbers are valid for the respective banks.\\n\\nIn terms of presentation, once the data is extracted, organizing it into clear categories makes it easier to understand. Using bullet points or tables could help in presenting the information clearly.\\n\\nLet me try to outline the steps I would take:\\n\\n1. **Read the transaction data carefully**: Look for all the details provided.\\n2. **Identify each field**: Extract the transaction ID, date, payer, payee, etc.\\n3. **Extract additional details**: Look for account numbers, banks, location, purpose, and status.\\n4. **Note any additional information**: Such as cross-border transfers, which adds context.\\n5. **Organize the extracted information**: Present it in a structured manner for clarity.\\n\\nI should also consider the possibility of missing information. For example, a transaction might not specify the purpose or the location. In such cases, it\\'s important to note that the information is unavailable or not provided.\\n\\nAdditionally, I should think about data formatting. For instance, the amount is given as 500,000,000 USD. It might be useful to format it as $500,000,000 for clarity. Similarly, the date is in ISO 8601 format, which is good for standardization.\\n\\nI wonder if there are any other fields that might be relevant. Maybe the transaction reference number, or the intermediary banks involved in a wire transfer. These could add more depth to the information extracted.\\n\\nAlso, considering the purpose of the transaction, it\\'s important to note if it\\'s for goods, services, investment, etc. This context can be vital for understanding the nature of the transaction.\\n\\nIn summary, extracting information from a transaction involves identifying and categorizing each piece of data, ensuring accuracy, and presenting it in a clear and organized manner. It\\'s also essential to consider any additional context or notes that might provide deeper insights into the transaction.\\n</think>\\n\\n**Transaction Information Extraction and Analysis**\\n\\nThe following structured breakdown organizes the extracted information from the given transaction, incorporating all relevant details and additional considerations:\\n\\n---\\n\\n**1. Transaction ID**\\n- TXN20250322113045\\n\\n**2. Date and Time**\\n- 2025-03-22T11:30:45Z (ISO 8601 format)\\n\\n**3. Payer Details**\\n- **Name**: Tesla, Inc.\\n- **Account Number**: TESLA987654321\\n- **Bank**: JPMorgan Chase Bank, USA\\n\\n**4. Payee Details**\\n- **Name**: Adani Green Energy Ltd.\\n- **Account Number**: ADANIGREEN123456\\n- **Bank**: State Bank of India, India\\n\\n**5. Transaction Amount**\\n- $500,000,000 USD\\n\\n**6. Payment Method**\\n- Wire Transfer\\n\\n**7. Purpose**\\n- Investment in Renewable Energy Collaboration\\n\\n**8. Status**\\n- Completed\\n\\n**9. Geographical Context**\\n- **Payer\\'s Bank Location**: USA\\n- **Payee\\'s Bank Location**: India\\n- **Note**: Cross-border transaction\\n\\n**10. Additional Notes**\\n- The transaction involves a significant investment in renewable energy, indicating a strategic partnership.\\n- The use of wire transfer suggests an international transaction, given the involvement of banks in different countries.\\n\\n---\\n\\nThis structured approach ensures clarity and comprehensiveness, providing a detailed overview of the transaction\\'s components and context.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 1478, 'prompt_tokens': 194, 'total_tokens': 1672, 'completion_time': 10.557142857, 'prompt_time': 0.052909847, 'queue_time': 0.11754220899999998, 'total_time': 10.610052704}, 'model_name': 'deepseek-r1-distill-qwen-32b', 'system_fingerprint': 'fp_d458a8aba5', 'finish_reason': 'stop', 'logprobs': None} id='run-e1ca5d14-75ee-479e-b8a9-371ba6803d06-0' usage_metadata={'input_tokens': 194, 'output_tokens': 1478, 'total_tokens': 1672}\n"
     ]

    }
   ],
   "source": [
    "# from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# transaction = input('Enter transaction: ')\n",
    "\n",
    "# messages = [\n",
    "#     SystemMessage(\"You are an assitant that helps data analysts in a financial institution to risk score a transaction among entities/corporations. It can also include individuals. Given an input transaction, you need to predict the risk score (0 to 1) of the transaction, confidence score (0 to 1) and a reason for the risk score.\"),\n",
    "#     HumanMessage(transaction),\n",
    "# ]\n",

    "## Update this with structured output? To get a JSON output??\n",
    "messages = [\n",
    "    SystemMessage(\"Given an input transaction, extract the entities involved, the value of the transaction, the date of the transaction, the type of transaction, the location and any other information that you think is relevant.\"),\n",
    "    HumanMessage(transaction),\n",
    "]\n",
    "output = model.invoke(messages)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RunnableSequence.invoke() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     16\u001b[39m chain = (\n\u001b[32m     17\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m: RunnablePassthrough()}\n\u001b[32m     18\u001b[39m     | prompt\n\u001b[32m     19\u001b[39m     | model\n\u001b[32m     20\u001b[39m     | StrOutputParser()\n\u001b[32m     21\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# chain = (\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m#     {\"context\": retriever, \"transaction\": RunnablePassthrough()},\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m#     | prompt\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m#     | model\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m#     | StrOutputParser()\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: RunnableSequence.invoke() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an agent whose output is the input for a GraphCyperChainQA that generates a query, queries and gives a output from a GraphDB. The database has relationships among different entities (companies, officers). Using the context generate human readable text that when given as an input to the subsequent chain the chain should generate good and useful responses. Use the context if needed.\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "chain = (\n",
    "    {\"output\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# chain = (\n",
    "#     {\"context\": retriever, \"transaction\": RunnablePassthrough()},\n",
    "#     | prompt\n",
    "#     | model\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "chain.invoke()"

   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Chain with prompt (chatprompttemplate) and context (wikipedia retriever) and model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikipedia Retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "\n",
    "# documentloader \n",
    "wiki_retriever = WikipediaRetriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = wiki_retriever.invoke(\"Adani Green Energy\")\n",
    "# for doc in docs:\n",
    "#     print(doc)\n",
    "#     print(\"===\")\n",
    "\n",
    "# print(docs[2].page_content[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(\n",
    "#     \"\"\"\n",
    "#     You are an agent that helps data analysts in a financial institution by risk scoring a transaction \n",
    "#     among entities/corporations. It can also include individuals. Given an input transaction, you need to output \n",
    "#     the risk score (0 to 1) of the transaction, confidence score (0 to 1). Use the context if needed.\n",
    "#     Context: {context}\n",
    "#     Transaction: {transaction}\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# chain = (\n",
    "#     {\"context\": wiki_retriever | format_docs, \"transaction\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | model\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "# # chain = (\n",
    "# #     {\"context\": retriever, \"transaction\": RunnablePassthrough()},\n",
    "# #     | prompt\n",
    "# #     | model\n",
    "# #     | StrOutputParser()\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # transaction_test = input()\n",
    "# chain.invoke(\n",
    "#     \"Transaction ID: TXN20250322113045, Date: 2025-03-22T11:30:45Z,  Payer: Tesla, Inc.  Payer Account Number: TESLA987654321,  Payer Bank: JPMorgan Chase Bank, USA, Payee: Adani Green Energy Ltd.  ,Payee Account Number: ADANIGREEN123456  ,Payee Bank: State Bank of India, India  ,Amount: 500,000,000 USD  ,Payment Method: Wire Transfer  ,Purpose: Investment in Renewable Energy Collaboration  ,Status: Completed  \"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "# graph = Neo4jGraph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph.refresh_schema()\n",
    "# print(graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhanced_graph = Neo4jGraph(enhanced_schema=True)\n",
    "# print(enhanced_graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Note: Sometimes this may return an error. It depends on the how well the llm is able to generate the cypher query. \n",
    "# ## Few-shot prompting is one way to improve the performance of the llm.\n",
    "# from langchain_neo4j import GraphCypherQAChain\n",
    "\n",
    "# chain = GraphCypherQAChain.from_llm(\n",
    "#     graph=enhanced_graph, llm=model, verbose=True, allow_dangerous_requests=True, validate_cypher=True\n",
    "# )\n",
    "# response = chain.invoke({\"query\": \"Name the officers that are linked with French entities\"})\n",
    "# print(response) # response becomes context \n",
    "\n",
    "\n",
    "# ## Input: Transaction -> Process the input to extract entities and generate a prompt to generate a query -> Query the graph database to get the required information and store it -> Pass this along with Wikipedia retriever to generate the risk_score and confidence score \n",
    "# ## Should we use separate chains? Is it possible to use one chain to do all of this?\n",
    "\n",
    "# ## Output: Risk Score, Confidence Score, Reason for the risk score\n",
    "\n",
    "# ## catching error, timeout ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "api_key = input(\"Enter your OpenSanctions API Key: \")\n",
    "os.environ[\"OPENSANCTIONS_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# import requests\n",
    "# import os\n",
    "\n",
    "# OS_API_KEY = os.getenv(\"OPENSANCTIONS_API_KEY\")\n",
    "# if not OS_API_KEY:\n",
    "#     raise ValueError(\"The OS_API_KEY environment variable is not set\")\n",
    "\n",
    "# def fetch_sanctions_data(person_name, company_name):\n",
    "#     headers = {\"Authorization\": OS_API_KEY}\n",
    "\n",
    "#     query = {\n",
    "#         \"queries\": {\n",
    "#             \"query-A\": {\"schema\": \"Person\", \"properties\": {\"name\": [person_name]}},\n",
    "#             \"query-B\": {\"schema\": \"Company\", \"properties\": {\"name\": [company_name]}},\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     response = requests.post(\n",
    "#         \"https://api.opensanctions.org/match/default\", headers=headers, json=query\n",
    "#     )\n",
    "#     response.raise_for_status()\n",
    "#     response_json = response.json()\n",
    "\n",
    "#     print(\"\\nFull API Response:\")\n",
    "#     pprint(response_json, sort_dicts=False)\n",
    "\n",
    "#     # Define known risky datasets\n",
    "#     risky_datasets = {\n",
    "#         \"us_ofac_sdn\", \"eu_travel_bans\", \"gb_hmt_sanctions\", \"ua_nsdc_sanctions\",\n",
    "#         \"eu_fsf\", \"lt_fiu_freezes\", \"ca_dfatd_sema_sanctions\", \"jp_mof_sanctions\",\n",
    "#         \"ch_seco_sanctions\", \"ru_acf_bribetakers\", \"be_fod_sanctions\", \"au_dfat_sanctions\"\n",
    "#     }\n",
    "    \n",
    "#     for query_id, query_response in response_json[\"responses\"].items():\n",
    "#         print(f\"\\nResults for query {query_id}:\")\n",
    "#         results = []\n",
    "        \n",
    "#         for result in query_response[\"results\"]:\n",
    "#             entity_info = {\n",
    "#                 \"id\": result[\"id\"],\n",
    "#                 \"name\": result[\"properties\"].get(\"name\", []),\n",
    "#                 \"match\": result[\"match\"],\n",
    "#                 \"topics\": result[\"properties\"].get(\"topics\", []),\n",
    "#                 \"datasets\": result.get(\"datasets\", []),\n",
    "#             }\n",
    "#             results.append(entity_info)\n",
    "\n",
    "#         pprint(results, sort_dicts=False)\n",
    "\n",
    "#         # **Improved Risk Detection**\n",
    "#         risky_entities = []\n",
    "#         for entity in results:\n",
    "#             entity_topics = set(entity.get(\"topics\", []))\n",
    "#             entity_datasets = set(entity.get(\"datasets\", []))\n",
    "\n",
    "#             if entity_topics.intersection({\"sanction\", \"pep\", \"crime\", \"corruption\", \"terrorism\"}) or \\\n",
    "#                entity_datasets.intersection(risky_datasets):\n",
    "#                 risky_entities.append(entity)\n",
    "\n",
    "#         if risky_entities:\n",
    "#             print(\"\\nüö® Risky Entities Detected:\")\n",
    "#             pprint(risky_entities, sort_dicts=False)\n",
    "#         else:\n",
    "#             print(\"\\n‚úÖ No risky entities found.\")\n",
    "\n",
    "# # Example Usage:\n",
    "# fetch_sanctions_data(\"Vladimir Putin\", \"\")\n",
    "\n",
    "\n",
    "\n",
    "# # \"name\": result[\"properties\"].get(\"name\", []),\n",
    "#                     # \"match\": result[\"match\"],\n",
    "#                     # \"topics\": list(entity_topics),  \n",
    "#                     # \"datasets\": list(entity_datasets),  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# import requests\n",
    "# import os\n",
    "\n",
    "# OS_API_KEY = os.getenv(\"OPENSANCTIONS_API_KEY\")\n",
    "# if not OS_API_KEY:\n",
    "#     raise ValueError(\"The OS_API_KEY environment variable is not set\")\n",
    "\n",
    "# def fetch_sanctions_data(person_name, company_name):\n",
    "#     headers = {\"Authorization\": OS_API_KEY}\n",
    "\n",
    "#     query = {\n",
    "#         \"queries\": {\n",
    "#             \"query-A\": {\"schema\": \"Person\", \"properties\": {\"name\": [person_name]}},\n",
    "#             \"query-B\": {\"schema\": \"Company\", \"properties\": {\"name\": [company_name]}},\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     response = requests.post(\n",
    "#         \"https://api.opensanctions.org/match/default\", headers=headers, json=query\n",
    "#     )\n",
    "#     response.raise_for_status()\n",
    "#     response_json = response.json()\n",
    "\n",
    "#     print(\"\\nFull API Response:\")\n",
    "#     pprint(response_json, sort_dicts=False)\n",
    "\n",
    "#     # Define known risky datasets and topics\n",
    "#     risky_datasets = {\n",
    "#     \"eu_travel_bans\", \"gb_hmt_sanctions\", \"ua_nsdc_sanctions\", \"jp_mof_sanctions\",\n",
    "#     \"ca_dfatd_sema_sanctions\", \"ch_seco_sanctions\", \"lt_fiu_freezes\", \"au_dfat_sanctions\",\n",
    "#     \"us_ofac_sdn\", \"ru_acf_bribetakers\", \"be_fod_sanctions\", \"eu_fsf\",\n",
    "#     \"in_nse_debarred\"  # ‚úÖ Added NSE debarred list\n",
    "# }\n",
    "\n",
    "\n",
    "#     risky_topics = {\"wanted\", \"sanction\", \"poi\", \"corruption\", \"crime\", \"role.pep\", \"terrorism\", \"reg.warn\"}\n",
    "\n",
    "\n",
    "#     for query_id, query_response in response_json[\"responses\"].items():\n",
    "#         print(f\"\\nResults for query {query_id}:\")\n",
    "#         results = []\n",
    "        \n",
    "#         for result in query_response[\"results\"]:\n",
    "#             entity_topics = set(result[\"properties\"].get(\"topics\", []))  # Fix extraction\n",
    "#             entity_datasets = set(result.get(\"datasets\", []))  # Fix extraction\n",
    "\n",
    "#             print(f\"\\nüîé Checking Entity: {result['id']}\")\n",
    "#             print(f\"üìå Topics: {entity_topics}\")\n",
    "#             print(f\"üìå Datasets: {entity_datasets}\")\n",
    "\n",
    "#             entity_info = {\n",
    "#                 \"id\": result[\"id\"],\n",
    "#                 \"name\": result[\"properties\"].get(\"name\", []),\n",
    "#                 \"match\": result[\"match\"],\n",
    "#                 \"topics\": list(entity_topics),  \n",
    "#                 \"datasets\": list(entity_datasets),  \n",
    "#             }\n",
    "#             results.append(entity_info)\n",
    "\n",
    "#         pprint(results, sort_dicts=False)\n",
    "\n",
    "#         # **Improved Risk Detection**\n",
    "#         risky_entities = []\n",
    "#         for entity in results:\n",
    "#             entity_topics = set(entity[\"topics\"])\n",
    "#             entity_datasets = set(entity[\"datasets\"])\n",
    "\n",
    "#             print(f\"\\nüßê Checking Risk for {entity['id']}:\")\n",
    "#             print(f\"üü° Entity Topics: {entity_topics}\")\n",
    "#             print(f\"üü° Risky Topics: {risky_topics}\")\n",
    "#             print(f\"üîπ Matching Topics: {entity_topics.intersection(risky_topics)}\")\n",
    "\n",
    "#             print(f\"üü¢ Entity Datasets: {entity_datasets}\")\n",
    "#             print(f\"üü¢ Risky Datasets: {risky_datasets}\")\n",
    "#             print(f\"üî∏ Matching Datasets: {entity_datasets.intersection(risky_datasets)}\")\n",
    "\n",
    "#             if entity_topics.intersection(risky_topics) or entity_datasets.intersection(risky_datasets):\n",
    "#                 risky_entities.append(entity)\n",
    "\n",
    "#         if risky_entities:\n",
    "#             print(\"\\nüö® Risky Entities Detected:\")\n",
    "#             pprint(risky_entities, sort_dicts=False)\n",
    "#         else:\n",
    "#             print(\"\\n‚úÖ No risky entities found.\")\n",
    "\n",
    "# # Example Usage:\n",
    "# fetch_sanctions_data(\"\",  \"Adani Green Energy Ltd.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added changes to the branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema.retriever import BaseRetriever\n",
    "\n",
    "class OpenSanctionsRetriever(BaseRetriever):\n",
    "\n",
    "    def _get_relevant_documents(self, query):\n",
    "        \"\"\"\n",
    "        Queries OpenSanctions API and returns relevant documents.\n",
    "        \"\"\"\n",
    "\n",
    "        person_name=\"\"\n",
    "        company_name=\"\"\n",
    "        api_key = \"3b9678eb2e0dff14c268b43f7acf4798\"\n",
    "        if \"Company:\" in query or \"Person:\" in query:\n",
    "            parts = query.split(\",\")\n",
    "            for part in parts:\n",
    "                if \"Company:\" in part:\n",
    "                    company_name = part.split(\"Company:\")[-1].strip()\n",
    "                    print(company_name)\n",
    "                elif \"Person:\" in part:\n",
    "                    person_name = part.split(\"Person:\")[-1].strip()\n",
    "                    print(person_name)\n",
    "\n",
    "\n",
    "        headers = {\"Authorization\": api_key}\n",
    "        # params = {\"q\": query}\n",
    "\n",
    "        query = {\n",
    "            \"queries\": {\n",
    "                \"query-A\": {\"schema\": \"Person\", \"properties\": {\"name\": [person_name]}},\n",
    "                \"query-B\": {\"schema\": \"Company\", \"properties\": {\"name\": [company_name]}},\n",
    "            }\n",
    "        }\n",
    "        response = requests.post(\n",
    "            \"https://api.opensanctions.org/match/default\", headers=headers, json=query\n",
    "        )\n",
    "        # if response.status_code != 200:\n",
    "        #     return []\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        response_json = response.json()\n",
    "\n",
    "        # print(\"\\nFull API Response:\")\n",
    "        # pprint(response_json, sort_dicts=False)\n",
    "\n",
    "        # if not response_json.get(\"results\"):\n",
    "        #     print(\"empty list lool\")\n",
    "        #     return []\n",
    "\n",
    "        documents = []\n",
    "        # print(\"outside first for\")\n",
    "        for query_id, query_response in response_json[\"responses\"].items():\n",
    "            # print(f\"\\nResults for query {query_id}:\")\n",
    "            # results = []\n",
    "            \n",
    "            for result in query_response[\"results\"]:\n",
    "                # print(\"in for result\")\n",
    "                entity_topics = set(result[\"properties\"].get(\"topics\", []))  # Fix extraction\n",
    "                entity_datasets = set(result.get(\"datasets\", []))  # Fix extraction\n",
    "\n",
    "                # print(f\"\\nüîé Checking Entity: {result['id']}\")\n",
    "                # print(f\"üìå Topics: {entity_topics}\")\n",
    "                # print(f\"üìå Datasets: {entity_datasets}\")\n",
    "                \n",
    "                name_to_store_page_content=result[\"properties\"].get(\"name\")\n",
    "                # print(\"hehe\", name_to_store_page_content[0])\n",
    "                entity_info = {\n",
    "                    \"id\": result[\"id\"],\n",
    "                    \"name\": result[\"properties\"].get(\"name\", []),\n",
    "                    \"match\": result[\"match\"],\n",
    "                    \"topics\": list(entity_topics),  \n",
    "                    \"datasets\": list(entity_datasets),  \n",
    "                }\n",
    "                doc = Document(page_content=f\"Sanctions data for {name_to_store_page_content}\", metadata=entity_info)\n",
    "                # print(\"doc = \", doc)\n",
    "                documents.append(doc)\n",
    "                # results.append(entity_info)\n",
    "        return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenSanctions_retriever = OpenSanctionsRetriever()\n",
    "# sanction_docs = OpenSanctions_retriever._get_relevant_documents(\"Company: Adani Green Energy Ltd., Person: Adani\")\n",
    "# print(\"sanction_docs = \", sanction_docs)\n",
    "# # print(sanction_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc in sanction_docs:\n",
    "#     print(doc)\n",
    "#     print(\"===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = ensemble_retriever.invoke(\"Company: Adani Green Energy Ltd., Person: Adani\")\n",
    "# docs\n",
    "# # for doc in docs:\n",
    "# #     print(doc.metadata)\n",
    "# #     print(doc.page_content)\n",
    "# #     print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/n743976x54b5sf64vktc7md40000gn/T/ipykernel_17730/999324814.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Step 1: Load and vectorize Instructions.md\n",
    "loader = TextLoader(\"Instructions.md\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Use a local embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Store Instructions.md in FAISS and create a retriever\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "instructions_retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[wiki_retriever, OpenSanctions_retriever, instructions_retriever], weights=[0.25, 0.5, 0.25]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt_test = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an agent that helps data analysts in a financial institution by risk scoring a transaction \n",
    "    among entities/corporations. It can also include individuals. Given an input transaction, you need to output \n",
    "    the risk score (0 to 1) of the transaction, confidence score (0 to 1) and reason for your answer.\n",
    "    Context: {context}\n",
    "    Transaction: {transaction}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "chain_test = (\n",
    "    {\"context\": ensemble_retriever | format_docs, \"transaction\": RunnablePassthrough()}\n",
    "    | prompt_test\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided transaction details, let\\'s perform the risk scoring analysis:\\n\\n### **Extracted Key Information**\\n\\n1. **Key Entity Information:**\\n   - **Sender:** Tesla, Inc.\\n     - **Entity Type:** Corporation\\n     - **Account Number:** TESLA987654321\\n   - **Receiver:** Adani Green Energy Ltd.\\n     - **Entity Type:** Corporation\\n     - **Account Number:** ADANIGREEN123456\\n   - **Intermediary Bank:** JPMorgan Chase Bank, USA\\n   - **Final Bank:** State Bank of India, India\\n\\n2. **Transaction Details:**\\n   - **Amount:** 500,000,000 USD\\n   - **Transaction Type:** Wire Transfer\\n   - **Purpose:** Investment in Renewable Energy Collaboration\\n   - **Completion Time:** 2025-03-22T11:30:45Z\\n\\n3. **Additional Risk Indicators:**\\n   - **Intermediaries & Routing:** The transaction involves a US bank (JPMorgan Chase) and an Indian bank (State Bank of India).\\n   - **Sanctions/Watchlists:** Tesla, Inc. and Adani Green Energy Ltd. need to be checked against sanctions lists.\\n   - **Country Risk:** India is considered a somewhat high-risk jurisdiction for financial transactions due to its involvement in several high-profile investigations and regulatory issues.\\n   - **Transaction Patterns:** The transaction amount is significant, but the purpose seems legitimate (Investment in Renewable Energy Collaboration).\\n\\n### **Risk Analysis**\\n\\n1. **Sanctioned Entity Involvement**\\n   - **Risk Weight:** 1.0\\n   - **Confidence Score:** 0.2 (Neither Tesla nor Adani Green Energy Ltd. appears on major sanctions lists, but Adani Group has faced scrutiny, so some caution is warranted.)\\n   - **Reason:** No direct involvement found on major sanctions lists, but Adani Group has faced regulatory scrutiny.\\n\\n2. **Shell Company Activity**\\n   - **Risk Weight:** 0.9\\n   - **Confidence Score:** 0.1 (No indications of shell company activity found for either entity.)\\n   - **Reason:** No known shell company associations for Tesla or Adani Green Energy Ltd.\\n\\n3. **Politically Exposed Person (PEP) Involvement**\\n   - **Risk Weight:** 0.8\\n   - **Confidence Score:** 0.1 (No PEP involvement found in available records.)\\n   - **Reason:** Neither entity shows any connection to known PEPs.\\n\\n4. **Unusual Transaction Patterns**\\n   - **Risk Weight:** 0.7\\n   - **Confidence Score:** 0.4 (The transaction amount is large, but the purpose seems legitimate.)\\n   - **Reason:** Large transaction amount, but the stated purpose appears valid.\\n\\n5. **High-Risk Jurisdiction**\\n   - **Risk Weight:** 0.6\\n   - **Confidence Score:** 0.5 (India is considered a high-risk jurisdiction.)\\n   - **Reason:** India is listed as a high-risk jurisdiction by some financial regulators.\\n\\n6. **Lack of Transparency**\\n   - **Risk Weight:** 0.5\\n   - **Confidence Score:** 0.2 (No clear evidence of missing documents or forged information.)\\n   - **Reason:** No indication of missing or forged documents.\\n\\n7. **Use of Intermediaries in High-Risk Locations**\\n   - **Risk Weight:** 0.4\\n   - **Confidence Score:** 0.1 (No unnecessary intermediaries detected.)\\n   - **Reason:** The transaction uses a US bank and an Indian bank, both of which are reputable.\\n\\n8. **Mismatch Between Entity Type & Transaction**\\n   - **Risk Weight:** 0.3\\n   - **Confidence Score:** 0.1 (No mismatch found between entity types and transaction purpose.)\\n   - **Reason:** The transaction purpose aligns with the entity types (both are corporations involved in investment).\\n\\n9. **VPN or Proxy Usage**\\n   - **Risk Weight:** 0.2\\n   - **Confidence Score:** 0.1 (No VPN or proxy usage detected.)\\n   - **Reason:** No evidence of VPN or proxy usage in the transaction details.\\n\\n10. **Minor Inconsistencies**\\n    - **Risk Weight:** 0.1\\n    - **Confidence Score:** 0.1 (No minor inconsistencies found.)\\n    - **Reason:** No minor inconsistencies found in the provided data.\\n\\n### **Risk Score Calculation**\\n\\n\\\\[\\n\\\\text{Risk Score} = \\\\frac{(1.0 \\\\times 0.2) + (0.9 \\\\times 0.1) + (0.8 \\\\times 0.1) + (0.7 \\\\times 0.4) + (0.6 \\\\times 0.5) + (0.5 \\\\times 0.2) + (0.4 \\\\times 0.1) + (0.3 \\\\times 0.1) + (0.2 \\\\times 0.1) + (0.1 \\\\times 0.1)}{10}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Risk Score} = \\\\frac{0.2 + 0.09 + 0.08 + 0.28 + 0.3 + 0.1 + 0.04 + 0.03 + 0.02 + 0.01}{10} = \\\\frac{1.15}{10} = 0.115\\n\\\\]\\n\\n### **Final Confidence Score Calculation**\\n\\n\\\\[\\n\\\\text{Final Confidence Score} = \\\\frac{0.2 + 0.1 + 0.1 + 0.4 + 0.5 + 0.2 + 0.1 + 0.1 + 0.1 + 0.1}{10} = \\\\frac{2.0}{10} = 0.2\\n\\\\]\\n\\n### **Transaction Classification**\\n\\n- **Risk Score:** 0.115\\n- **Final Confidence Score:** 0.2\\n- **Risk Category:** Minimal Risk\\n- **Action:** No action needed.\\n\\n### **Output JSON**\\n\\n```json\\n{\\n  \"Transaction ID\": \"TXN20250322113045\",\\n  \"Extracted Entity\": [\"Tesla, Inc.\", \"Adani Green Energy Ltd.\"],\\n  \"Entity Type\": [\"Corporation\", \"Corporation\"],\\n  \"Risk Score\": 0.115,\\n  \"Supporting Evidence\": [\"OFAC Sanctions List\", \"Company Website\", \"Global Financial Integrity Report\"],\\n  \"Confidence Score\": 0.2,\\n  \"Reason\": \"No direct involvement in sanctioned entities, no shell company activity, no PEP connections, and the transaction amount is large but the purpose is legitimate. India is a high-risk jurisdiction, but there\\'s no strong evidence of illicit activity.\",\\n  \"Transaction Classification\": \"Minimal Risk\",\\n  \"Conclusion\": \"The transaction involves entities from a high-risk jurisdiction, but there is no direct evidence of involvement in illicit activities. The minimal risk score of 0.115 indicates no immediate action is required.\"\\n}\\n```\\n\\nThis analysis concludes that the transaction is categorized under minimal risk, requiring no immediate action, but the high-risk jurisdiction of India warrants continued monitoring.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_test.invoke(\"TXN20250322113045,2025-03-22T11:30:45Z,Tesla, Inc.,TESLA987654321,JPMorgan Chase Bank, USA,Adani Green Energy Ltd.,ADANIGREEN123456,State Bank of India, India,500000000,USD,Wire Transfer,Investment in Renewable Energy Collaboration,Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
